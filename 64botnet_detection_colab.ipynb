{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"21iDKjFWQLz3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766295910851,"user_tz":-420,"elapsed":32475,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}},"outputId":"c4204d04-83b8-4a98-f2aa-8f968ec3d88c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[AUTO-DETECT] Running on Google Colab\n","Mounted at /content/drive\n","[COLAB MODE] Đã mount Drive. Thư mục làm việc: /content/drive/MyDrive/botnet\n"]}],"source":["# ==========================================\n","# CONFIGURATION: IS_COLAB\n","# ==========================================\n","#\n","# **IMPORTANT: Auto-detects Colab, or you can override manually**\n","#\n","# - Auto-detection: Checks if 'google.colab' module exists\n","# - Override: Set IS_COLAB = True (Colab) or IS_COLAB = False (local)\n","#\n","# This controls:\n","# - Google Drive mounting (Colab only)\n","# - Working directory (Drive vs local)\n","# - Number of DataLoader workers (0 for Colab/Windows, 4 for Linux/Mac)\n","#\n","\n","# Auto-detect Colab environment\n","try:\n","    import google.colab\n","    IS_COLAB = True\n","    print(\"[AUTO-DETECT] Running on Google Colab\")\n","except ImportError:\n","    IS_COLAB = False\n","    print(\"[AUTO-DETECT] Running on local machine\")\n","\n","# OPTIONAL: Uncomment to manually override auto-detection\n","# IS_COLAB = True   # Force Colab mode\n","# IS_COLAB = False  # Force local mode\n","\n","# ==========================================\n","# CONNECT TO GOOGLE DRIVE (COLAB)\n","# ==========================================\n","if IS_COLAB:\n","    try:\n","        import os\n","        from google.colab import drive\n","\n","        drive.mount('/content/drive')\n","        # Thư mục làm việc trên Drive (tùy chỉnh theo ý bạn)\n","        WORKING_DIR = '/content/drive/MyDrive/botnet'\n","        os.makedirs(WORKING_DIR, exist_ok=True)\n","        os.chdir(WORKING_DIR)\n","        print(f\"[COLAB MODE] Đã mount Drive. Thư mục làm việc: {os.getcwd()}\")\n","    except Exception as e:\n","        print(f\"[WARNING] Lỗi khi mount Drive: {e}\")\n","        WORKING_DIR = '.'\n","else:\n","    import os\n","    # Local mode: Use current directory\n","    WORKING_DIR = '.'\n","    print(f\"[LOCAL MODE] Thư mục làm việc: {os.path.abspath(WORKING_DIR)}\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WjH3VFNoQLz5","executionInfo":{"status":"ok","timestamp":1766295910859,"user_tz":-420,"elapsed":3,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# # Install requirements\n","# !pip install torchinfo>=1.8.0 numpy>=1.24.0 pandas>=2.0.0 scikit-learn>=1.4.0,<1.5.0 imbalanced-learn>=0.12.0 torch torchvision matplotlib>=3.7.0 seaborn>=0.12.0 tqdm>=4.65.0 psutil>=5.9.0"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eIl1ORU_QLz5","executionInfo":{"status":"ok","timestamp":1766295923213,"user_tz":-420,"elapsed":12352,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# =============================\n","# GLOBAL IMPORTS\n","# =============================\n","import os\n","import gc\n","import ssl\n","import argparse\n","import pickle\n","import urllib.request\n","from collections import Counter\n","\n","import numpy as np\n","import pandas as pd\n","import psutil\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.utils import class_weight\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    confusion_matrix,\n","    classification_report,\n",")\n","from imblearn.over_sampling import SMOTE\n","\n","try:\n","    from torchinfo import summary\n","except ImportError:\n","    summary = None\n","\n","# Bypass SSL verification for dataset downloads\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"gCaUwKCgQLz6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766295923241,"user_tz":-420,"elapsed":14,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}},"outputId":"84e0e363-5625-4515-9cc3-c77d415e2906"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CONFIG] Colab detected: N_WORKERS = 0\n"]}],"source":["# ==========================================\n","# CONTENT FROM: config.py\n","# ==========================================\n","import os\n","import sys\n","import torch\n","\n","# =============================================================================\n","# SET WORKING DIRECTORY\n","# =============================================================================\n","# Use existing WORKING_DIR if defined (e.g. from cell 1), else default to '.'\n","\n","# =============================================================================\n","# SYSTEM RESOURCES & DEVICE\n","# =============================================================================\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# =============================================================================\n","# CONFIGURATION\n","# =============================================================================\n","# OPTIMIZED: Increase batch size to utilize 10GB spare GPU memory\n","BATCH_SIZE = 512  # Increased from 256 to 512 for better GPU utilization\n","\n","# DataLoader Workers: Auto-detect based on platform and environment\n","# - Colab: 0 (multiprocessing issues)\n","# - Windows: 0 (multiprocessing issues with CUDA)\n","# - Linux/Mac: 4 (safe to use multiprocessing)\n","try:\n","    if IS_COLAB:\n","        N_WORKERS = 0\n","        print(\"[CONFIG] Colab detected: N_WORKERS = 0\")\n","    elif sys.platform == 'win32':\n","        N_WORKERS = 0\n","        print(\"[CONFIG] Windows detected: N_WORKERS = 0 (multiprocessing disabled)\")\n","    else:\n","        N_WORKERS = 4\n","        print(\"[CONFIG] Linux/Mac detected: N_WORKERS = 4\")\n","except NameError:\n","    # If IS_COLAB not defined, assume safe default\n","    N_WORKERS = 0\n","    IS_COLAB = False\n","    print(\"[CONFIG] IS_COLAB not defined: defaulting to N_WORKERS = 0\")\n","\n","# Check RAM (optional logic can be here or main)\n","import psutil\n","ram_gb = psutil.virtual_memory().total / 1e9\n","\n","# Training config\n","N_EPOCHS = 20  # Tăng từ 6 lên 20 để model học tốt hơn\n","LEARNING_RATE = 0.001  # Tăng từ 0.0001 lên 0.001\n","IMAGE_SIZE = 64\n","\n","# OPTIMIZATION FLAGS\n","USE_AMP = True  # Enable automatic mixed precision for faster training\n","PIN_MEMORY = True  # Enable pinned memory for faster GPU transfers\n","\n","# =============================================================================\n","# DEFINING SCENARIOS\n","# =============================================================================\n","# Train scenarios:\n","TRAIN_SCENARIOS = ['8']\n","\n","# Test scenario: Rbot\n","TEST_SCENARIOS = ['10']\n","\n","# =============================================================================\n","# LABEL MAPPING (2 CLASSES: Botnet bao gồm cả C&C)\n","# =============================================================================\n","CLASS_TO_IDX = {\n","    'Botnet': 0,  # Gộp cả C&C vào đây vì C&C cũng là botnet traffic\n","    'Normal': 1\n","}\n","\n","# Inverse mapping\n","IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":true,"id":"MaxKC_s-QLz6","executionInfo":{"status":"ok","timestamp":1766295923260,"user_tz":-420,"elapsed":18,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: utils.py\n","# ==========================================\n","import os\n","import ssl\n","import urllib.request\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","# Bypass SSL verification\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","def create_directory(path):\n","    os.makedirs(path, exist_ok=True)\n","    print(f\"  Created/Checked: {path}\")\n","\n","def download_file(url, destination):\n","    try:\n","        if os.path.exists(destination):\n","            print(f\"  [SKIP] File exists: {os.path.basename(destination)}\")\n","            return True\n","\n","        print(f\"  Downloading: {os.path.basename(destination)}\")\n","        with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=\"  Progress\") as t:\n","            def reporthook(blocknum, blocksize, totalsize):\n","                t.total = totalsize\n","                t.update(blocknum * blocksize - t.n)\n","            urllib.request.urlretrieve(url, destination, reporthook=reporthook)\n","        return True\n","    except Exception as e:\n","        print(f\"Error downloading {url}: {str(e)}\")\n","        return False\n","\n","def check_csv_in_folder(folder_path):\n","    if not os.path.exists(folder_path): return False\n","    for file in os.listdir(folder_path):\n","        if file.endswith('.csv'): return True\n","    return False\n","\n","def rename(path_file, new_name):\n","    dir_path = os.path.dirname(path_file)\n","    path_new_name = os.path.join(dir_path, new_name)\n","    os.rename(path_file, path_new_name)\n","\n","def get_csv_paths(main_dir, scenario_ids):\n","    csv_paths = []\n","    for sid in scenario_ids:\n","        path = os.path.join(main_dir, sid)\n","        # Find csv file in folder\n","        if os.path.exists(path):\n","            for file in os.listdir(path):\n","                if file.endswith('.csv'):\n","                    csv_paths.append(os.path.join(path, file))\n","                    break\n","    return csv_paths\n","\n","def plot_and_save_loss(train_losses, valid_losses, save_path):\n","    \"\"\"\n","    Plots Loss chart and saves to file.\n","    \"\"\"\n","    plt.figure(figsize=(10, 6))\n","\n","    # Plot Train Loss\n","    plt.plot(train_losses, label='Train Loss', color='blue', marker='o', markersize=4)\n","\n","    # Plot Valid Loss\n","    plt.plot(valid_losses, label='Valid Loss', color='orange', marker='o', markersize=4)\n","\n","    # Decorate\n","    plt.title('Training vs Validation Loss', fontsize=16)\n","    plt.xlabel('Epochs', fontsize=12)\n","    plt.ylabel('Loss', fontsize=12)\n","    plt.legend(fontsize=12)\n","    plt.grid(True, linestyle='--', alpha=0.7)\n","\n","    # Save image\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    print(f\"\\n[INFO] Saved Loss plot at: {save_path}\")\n","\n","    # Show (optional in script mode, but harmless)\n","    # plt.show()\n","    plt.close()\n"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"id":"2DiTtBURQLz6","executionInfo":{"status":"ok","timestamp":1766295923272,"user_tz":-420,"elapsed":5,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: preprocessing_utils.py\n","# ==========================================\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import os\n","from tqdm.auto import tqdm\n","\n","def quick_classify(label):\n","    \"\"\"\n","    Classifies a label string into 'Botnet' or 'Normal'.\n","    UPDATED: Gộp C&C vào Botnet (C&C cũng là botnet traffic)\n","    \"\"\"\n","    if not isinstance(label, str):\n","        return 'Normal'\n","    label_lower = label.lower()\n","    # Gộp C&C vào Botnet: kiểm tra 'botnet', 'cc', hoặc 'c&c'\n","    if 'botnet' in label_lower or 'cc' in label_lower or 'c&c' in label_lower:\n","        return 'Botnet'\n","    else:\n","        return 'Normal'\n","\n","def calculate_global_frequencies(csv_paths):\n","    \"\"\"\n","    Calculates global frequencies for IPs and Ports across all CSV files.\n","    Returns a dictionary of dictionaries.\n","    \"\"\"\n","    freqs = {\n","        'SrcAddr': {},\n","        'DstAddr': {},\n","        'Sport': {},\n","        'Dport': {}\n","    }\n","\n","    print(\"Scanning CSVs for frequencies...\")\n","    for path in tqdm(csv_paths, desc=\"Global Freqs\"):\n","        try:\n","            # Read specific columns to save memory\n","            # Note: low_memory=False helps with mixed types\n","            df = pd.read_csv(path, usecols=['SrcAddr', 'DstAddr', 'Sport', 'Dport'], low_memory=False)\n","\n","            for col in freqs:\n","                if col in df.columns:\n","                    # Value counts\n","                    vc = df[col].value_counts().to_dict()\n","                    for k, v in vc.items():\n","                        freqs[col][k] = freqs[col].get(k, 0) + v\n","        except Exception as e:\n","            print(f\"Warning: Could not process {path} for frequencies: {e}\")\n","\n","    return freqs\n","\n","def process_batch_fast_v2(chunk, freq_dicts, expected_columns=None):\n","    \"\"\"\n","    Processes a batch (DataFrame chunk).\n","    1. Cleans data.\n","    2. Encodes features (Frequencies, One-Hot).\n","    3. Aligns columns.\n","    Returns (X_values, y_values, columns_list).\n","    \"\"\"\n","    df = chunk.copy()\n","\n","    # 1. Label Processing\n","    if 'Label' in df.columns:\n","        y = df['Label'].apply(quick_classify)\n","        df = df.drop(columns=['Label'])\n","    else:\n","        y = None\n","\n","    # 2. Basic Cleaning\n","    if 'StartTime' in df.columns:\n","        df = df.drop(columns=['StartTime'])\n","\n","    # 3. Numeric Conversions\n","    # Columns that should be numeric\n","    numeric_cols = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes']\n","    for col in numeric_cols:\n","        if col in df.columns:\n","            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n","\n","    # 4. Feature Engineering: Frequencies\n","    for col in ['SrcAddr', 'DstAddr', 'Sport', 'Dport']:\n","        freq_col_name = f\"{col}_freq\"\n","        if col in df.columns and col in freq_dicts:\n","            # Map values. Using map with a dict.\n","            # Note: If types don't match (e.g. int vs str), map produces NaN.\n","            # We assume consistent types from read_csv.\n","            df[freq_col_name] = df[col].map(freq_dicts.get(col, {})).fillna(0)\n","        else:\n","            df[freq_col_name] = 0\n","\n","    # 5. Split IP thành 4 octet (4 feature riêng) thay vì một integer lớn\n","    for ip_col in ['SrcAddr', 'DstAddr']:\n","        if ip_col in df.columns:\n","            # Chuyển sang string để split theo '.'\n","            ip_str = df[ip_col].astype(str)\n","            parts = ip_str.str.split('.', expand=True)\n","\n","            # Đảm bảo có đủ 4 cột (nếu IP không chuẩn, phần thiếu sẽ là NaN)\n","            for i in range(4):\n","                if i < parts.shape[1]:\n","                    df[f'{ip_col}_octet_{i+1}'] = pd.to_numeric(parts[i], errors='coerce').fillna(0)\n","                else:\n","                    # Nếu thiếu cột, tạo cột toàn 0\n","                    df[f'{ip_col}_octet_{i+1}'] = 0\n","\n","    # 6. Handle Port Columns (Keep as numeric features)\n","    for col in ['Sport', 'Dport']:\n","        if col in df.columns:\n","            # Force numeric (handle hex or strings)\n","            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n","\n","    # 7. Drop raw IP addresses (đã có tần suất + 4 octet)\n","    df = df.drop(columns=['SrcAddr', 'DstAddr'], errors='ignore')\n","\n","    # 8. One-Hot Encoding for 'Proto'\n","    if 'Proto' in df.columns:\n","        # Common protos: TCP, UDP, ICMP\n","        # Limit cardinality if needed? Usually low.\n","        dummies = pd.get_dummies(df['Proto'], prefix='Proto')\n","        df = pd.concat([df, dummies], axis=1)\n","        df = df.drop(columns=['Proto'])\n","\n","    # 8. One-Hot Encoding for 'State' (không giới hạn top state)\n","    if 'State' in df.columns:\n","        dummies = pd.get_dummies(df['State'], prefix='State')\n","        df = pd.concat([df, dummies], axis=1)\n","        df = df.drop(columns=['State'])\n","\n","    # 9. Handle 'Dir'\n","    if 'Dir' in df.columns:\n","         dummies = pd.get_dummies(df['Dir'], prefix='Dir')\n","         df = pd.concat([df, dummies], axis=1)\n","         df = df.drop(columns=['Dir'])\n","\n","    # 10. Align columns with expected_columns\n","    if expected_columns is not None:\n","        # Add missing columns with 0\n","        # Use reindex for efficiency and handling both missing and extra\n","        df = df.reindex(columns=expected_columns, fill_value=0)\n","    else:\n","        # First time detection: Fill NaNs just in case\n","        df = df.fillna(0)\n","\n","    # Return values\n","    X_vals = df.values.astype(np.float32) # Ensure float32 for PyTorch\n","    y_vals = y.values if y is not None else None\n","\n","    return X_vals, y_vals, df.columns.tolist()\n","\n","def save_global_stats(global_stats, filepath='global_stats.pkl'):\n","    \"\"\"Saves global statistics to a pickle file.\"\"\"\n","    with open(filepath, 'wb') as f:\n","        pickle.dump(global_stats, f)\n","    print(f\"Global stats saved to {filepath}\")\n","\n","def load_global_stats(filepath='global_stats.pkl'):\n","    \"\"\"Loads global statistics from a pickle file.\"\"\"\n","    if not os.path.exists(filepath):\n","        raise FileNotFoundError(f\"{filepath} not found.\")\n","    with open(filepath, 'rb') as f:\n","        return pickle.load(f)\n","\n","def save_scaler(scaler, filepath='scaler.pkl'):\n","    \"\"\"Saves the scaler to a pickle file.\"\"\"\n","    with open(filepath, 'wb') as f:\n","        pickle.dump(scaler, f)\n","    print(f\"Scaler saved to {filepath}\")\n","\n","def load_scaler(filepath='scaler.pkl'):\n","    \"\"\"Loads the scaler from a pickle file.\"\"\"\n","    if not os.path.exists(filepath):\n","        raise FileNotFoundError(f\"{filepath} not found.\")\n","    with open(filepath, 'rb') as f:\n","        return pickle.load(f)"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":true,"id":"Mr7W7QlPQLz7","executionInfo":{"status":"ok","timestamp":1766295923282,"user_tz":-420,"elapsed":6,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: loss.py\n","# ==========================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, weight=None, gamma=2., reduction='mean'):\n","        \"\"\"\n","        Args:\n","            weight (Tensor, optional): A manual rescaling weight given to each class.\n","                                       If given, has to be a Tensor of size C.\n","            gamma (float): Focusing parameter.\n","            reduction (string, optional): Specifies the reduction to apply to the output:\n","                                          'none' | 'mean' | 'sum'. 'mean': the sum of the output will be divided by the number of elements in the output,\n","                                          'sum': the output will be summed. Default: 'mean'\n","        \"\"\"\n","        super(FocalLoss, self).__init__()\n","        self.weight = weight\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        \"\"\"\n","        Args:\n","            inputs: (N, C) where C = number of classes.\n","            targets: (N) where each value is 0 <= targets[i] <= C-1.\n","        \"\"\"\n","        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":true,"id":"Lv1gR3QAQLz7","executionInfo":{"status":"ok","timestamp":1766295923338,"user_tz":-420,"elapsed":44,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: model.py\n","# ==========================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BotnetImageCNN(nn.Module):\n","    \"\"\"\n","    Custom CNN train from scratch cho ảnh 1xNxN (N = 32, 64, hoặc 112).\n","    Không dùng pretrained, tối ưu cho network flow images.\n","    UPDATED: 2 classes (Botnet vs Normal), flexible image size\n","    \"\"\"\n","\n","    def __init__(self, image_size: int = 64, n_classes: int = 2):\n","        super().__init__()\n","\n","        self.image_size = image_size\n","\n","        # Tính số lượng pooling layers dựa trên image size\n","        # 32x32: 3 pooling -> 4x4\n","        # 64x64: 4 pooling -> 4x4\n","        # 112x112: 4 pooling -> 7x7\n","\n","        if image_size == 32:\n","            # Architecture for 32x32\n","            self.features = nn.Sequential(\n","                # Block 1: 32x32 -> 16x16\n","                nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(32),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(32),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.1),\n","\n","                # Block 2: 16x16 -> 8x8\n","                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.1),\n","\n","                # Block 3: 8x8 -> 4x4\n","                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.2),\n","\n","                # Block 4: 4x4 -> 1x1 (global pooling)\n","                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                nn.AdaptiveAvgPool2d(1)\n","            )\n","            feature_size = 256\n","\n","        elif image_size == 64:\n","            # Architecture for 64x64\n","            self.features = nn.Sequential(\n","                # Block 1: 64x64 -> 32x32\n","                nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(32),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(32),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.1),\n","\n","                # Block 2: 32x32 -> 16x16\n","                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.1),\n","\n","                # Block 3: 16x16 -> 8x8\n","                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.2),\n","\n","                # Block 4: 8x8 -> 4x4\n","                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.2),\n","\n","                # Block 5: 4x4 -> 1x1 (global pooling)\n","                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),\n","                nn.AdaptiveAvgPool2d(1)\n","            )\n","            feature_size = 512\n","\n","\n","        elif image_size == 128:\n","            # Architecture for 128x128\n","            self.features = nn.Sequential(\n","                # Block 1: 128x128 -> 64x64\n","                nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(32),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(32, 32, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(32),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.1),\n","\n","                # Block 2: 64x64 -> 32x32\n","                nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.1),\n","\n","                # Block 3: 32x32 -> 16x16\n","                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(128),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.15),\n","\n","                # Block 4: 16x16 -> 8x8\n","                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(256),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.2),\n","\n","                # Block 5: 8x8 -> 4x4\n","                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(2, 2),\n","                nn.Dropout2d(0.2),\n","\n","                # Block 6: 4x4 -> 1x1 (global pooling)\n","                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(512),\n","                nn.ReLU(inplace=True),\n","                nn.AdaptiveAvgPool2d(1)\n","            )\n","            feature_size = 512\n","\n","        else:\n","            raise ValueError(f\"Unsupported image_size: {image_size}. Must be 32, 64, 112, or 128.\")\n","\n","        # Classifier (same for all sizes)\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(feature_size, 128),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, n_classes)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # x: [B, 1, N, N] where N = image_size\n","        x = self.features(x)  # [B, feature_size, 1, 1]\n","        x = x.view(x.size(0), -1)  # [B, feature_size]\n","        x = self.classifier(x)  # [B, n_classes]\n","        return x\n","\n","\n","class BotnetClassifier(nn.Module):\n","    \"\"\"\n","    Wrapper giữ API cũ:\n","      - vẫn nhận n_features cho tương thích nhưng không dùng; dữ liệu đã là ảnh.\n","    UPDATED: Default 2 classes, flexible image size\n","    \"\"\"\n","\n","    def __init__(self, base_model=None, n_features=None, image_size: int = 64, n_classes: int = 2):\n","        super().__init__()\n","        self.model = BotnetImageCNN(image_size=image_size, n_classes=n_classes)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.model(x)"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"id":"wYnCAsxbQLz7","executionInfo":{"status":"ok","timestamp":1766295923396,"user_tz":-420,"elapsed":57,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: data_loader.py\n","# ==========================================\n","import torch\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import numpy as np\n","from tqdm.auto import tqdm\n","\n","# from preprocessing_utils import process_batch_fast_v2  # Commented out for notebook compatibility\n","# from config import CLASS_TO_IDX, IMAGE_SIZE  # Commented out for notebook compatibility\n","\n","\n","class FastBotnetDataset(Dataset):\n","    \"\"\"\n","    Dataset:\n","      - Nhận X_data dạng [N, n_features]\n","      - Mỗi dòng được pad / cắt về 32x32 và reshape thành ảnh 1x32x32\n","      - FIXED: Global normalization thay vì per-sample\n","    \"\"\"\n","\n","    def __init__(self, X_data, y_data):\n","        self.X_data = torch.from_numpy(X_data).float()\n","        self.y_data = torch.from_numpy(y_data).long()\n","\n","        self.image_size = IMAGE_SIZE\n","        self.num_pixels = self.image_size * self.image_size\n","\n","        # Pre-compute Hilbert curve coordinates for mapping 1D features -> 2D ảnh\n","        self.hilbert_coords = self._hilbert_curve(self.image_size)\n","\n","        # Tính global statistics cho toàn dataset (để normalize consistently)\n","        print(\"Computing global statistics for image normalization...\")\n","        self._compute_global_stats()\n","\n","    def __len__(self):\n","        return len(self.X_data)\n","\n","    def _compute_global_stats(self):\n","        \"\"\"Tính percentiles toàn dataset để normalize consistently\"\"\"\n","        # Lấy sample 10% dữ liệu để tính stats nhanh hơn\n","        sample_size = min(10000, len(self.X_data))\n","        indices = torch.randperm(len(self.X_data))[:sample_size]\n","        sample_data = self.X_data[indices].flatten()\n","\n","        # Tính percentiles để clip outliers\n","        self.p1 = torch.quantile(sample_data, 0.01)\n","        self.p99 = torch.quantile(sample_data, 0.99)\n","\n","        # Tính mean/std cho ImageNet-style normalization\n","        clipped_data = torch.clamp(sample_data, min=self.p1, max=self.p99)\n","        self.global_mean = clipped_data.mean()\n","        self.global_std = clipped_data.std() + 1e-8\n","\n","        print(f\"  Global stats: p1={self.p1:.4f}, p99={self.p99:.4f}, mean={self.global_mean:.4f}, std={self.global_std:.4f}\")\n","\n","    def _hilbert_curve(self, n):\n","        \"\"\"\n","        Sinh danh sách tọa độ (row, col) theo đường cong Hilbert cho lưới n x n.\n","        n phải là lũy thừa của 2 (ở đây n = 32).\n","        Thuật toán đơn giản, ưu tiên tính dễ đọc hơn là tối ưu tuyệt đối.\n","        \"\"\"\n","        if n & (n - 1) != 0:\n","            raise ValueError(\"IMAGE_SIZE must be a power of 2 for Hilbert curve.\")\n","\n","        def hilbert_index_to_xy(d, order):\n","            \"\"\"\n","            Chuyển chỉ số 1D d trên Hilbert order 'order' (2^order x 2^order)\n","            sang tọa độ (x, y).\n","            \"\"\"\n","            x = y = 0\n","            t = d\n","            s = 1\n","            while s < (1 << order):\n","                rx = 1 & (t // 2)\n","                ry = 1 & (t ^ rx)\n","                # xoay / phản chiếu\n","                if ry == 0:\n","                    if rx == 1:\n","                        x, y = s - 1 - x, s - 1 - y\n","                    x, y = y, x\n","                x += s * rx\n","                y += s * ry\n","                t //= 4\n","                s <<= 1\n","            return x, y\n","\n","        order = int(np.log2(n))\n","        coords = []\n","        for d in range(n * n):\n","            x, y = hilbert_index_to_xy(d, order)\n","            coords.append((x, y))\n","        return coords\n","\n","    def __getitem__(self, idx):\n","        # vector đặc trưng 1D: [n_features]\n","        feature_vector = self.X_data[idx]\n","\n","        # Khởi tạo ảnh 0\n","        image = torch.zeros((1, self.image_size, self.image_size), dtype=feature_vector.dtype)\n","\n","        # Số feature thực sự có (không bỏ dòng nào, chỉ giới hạn số pixel)\n","        length = min(feature_vector.numel(), self.num_pixels)\n","\n","        # Map từng feature lên pixel theo Hilbert curve để giữ locality\n","        for i in range(length):\n","            x, y = self.hilbert_coords[i]\n","            image[0, x, y] = feature_vector[i]\n","\n","        # FIXED: Global normalization thay vì per-sample\n","        vals = image.view(-1)\n","\n","        # 1. Clip outliers sử dụng global percentiles\n","        vals = torch.clamp(vals, min=self.p1, max=self.p99)\n","\n","        # 2. Standardization sử dụng global mean/std\n","        vals = (vals - self.global_mean) / self.global_std\n","\n","        image = vals.view(1, self.image_size, self.image_size)\n","\n","        return image, self.y_data[idx]\n","\n","def load_data_from_csvs(csv_list, global_stats, desc=\"Loading\", is_train=True, scaler=None):\n","    \"\"\"\n","    Loads data from a list of CSVs, processes it, and returns X and y.\n","    \"\"\"\n","    X_list = []\n","    y_list = []\n","\n","    # Get stats\n","    freq_dicts = global_stats['freq_dicts']\n","    expected_cols = global_stats['expected_columns']\n","\n","    for csv_file in tqdm(csv_list, desc=desc):\n","        try:\n","            for chunk in pd.read_csv(csv_file, chunksize=100000, low_memory=False):\n","                X_batch, y_batch, _ = process_batch_fast_v2(\n","                    chunk, freq_dicts, expected_cols\n","                )\n","                if len(X_batch) > 0:\n","                    X_list.append(X_batch)\n","                    # Convert labels to indices\n","                    # y_batch is a Series or numpy array of strings\n","                    if y_batch is not None:\n","                         # Ensure y_batch contains valid keys from CLASS_TO_IDX\n","                        y_indices = np.array([CLASS_TO_IDX.get(label, 2) for label in y_batch]) # Default to Normal (2) if not found\n","                        y_list.append(y_indices)\n","        except Exception as e:\n","            print(f\"  Error loading {csv_file}: {e}\")\n","\n","    if not X_list:\n","        return np.array([]), np.array([])\n","\n","    X_data = np.vstack(X_list).astype(np.float32)\n","    y_data = np.concatenate(y_list).astype(np.int64)\n","    # 1. Sanitize input: Replace existing NaNs/Infs with 0\n","    X_data = np.nan_to_num(X_data, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","    # 2. Clip negative values to 0.\n","    # This prevents np.log1p(-1) from becoming -inf.\n","    X_data = np.maximum(X_data, 0)\n","\n","    # Normalize (Log1p + Scaler)\n","    X_data = np.log1p(X_data)\n","\n","    if scaler:\n","        if is_train:\n","            X_data = scaler.fit_transform(X_data)\n","        else:\n","            X_data = scaler.transform(X_data)\n","\n","    return X_data, y_data"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":true,"id":"X2n69c-3QLz8","executionInfo":{"status":"ok","timestamp":1766295923493,"user_tz":-420,"elapsed":97,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: analyze_csv.py\n","# ==========================================\n","import os\n","import argparse\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# from preprocessing_utils import quick_classify  # Commented out for notebook compatibility\n","\n","# Set style for plots\n","sns.set_theme(style=\"whitegrid\")\n","# Ensure matplotlib can display Vietnamese characters if supported font is found,\n","# otherwise fallback to default.\n","# In a standard env, this might be tricky, so we stick to standard fonts but write strings in Vietnamese.\n","\n","def load_data(filepath):\n","    \"\"\"Loads CSV data.\"\"\"\n","    if not os.path.exists(filepath):\n","        raise FileNotFoundError(f\"Không tìm thấy file: {filepath}\")\n","\n","    # Check if empty\n","    if os.path.getsize(filepath) == 0:\n","         raise ValueError(f\"File rỗng: {filepath}\")\n","\n","    print(f\"Đang tải dữ liệu từ: {filepath} ...\")\n","    # Low memory false to handle mixed types if any, similar to training script\n","    df = pd.read_csv(filepath, low_memory=False)\n","    return df\n","\n","def analyze_data(df):\n","    \"\"\"Calculates statistics and metrics.\"\"\"\n","    stats = {}\n","\n","    # 1. Label Mapping\n","    if 'Label' in df.columns:\n","        df['Mapped_Label'] = df['Label'].apply(quick_classify)\n","\n","        # Raw Label Counts\n","        stats['raw_label_counts'] = df['Label'].value_counts().head(20).to_dict()\n","\n","        # Mapped Label Counts\n","        stats['mapped_label_counts'] = df['Mapped_Label'].value_counts().to_dict()\n","        stats['mapped_label_pct'] = df['Mapped_Label'].value_counts(normalize=True).to_dict()\n","    else:\n","        stats['raw_label_counts'] = {}\n","        stats['mapped_label_counts'] = {}\n","        print(\"CẢNH BÁO: Không tìm thấy cột 'Label'.\")\n","\n","    # 2. Numerical Features Stats\n","    num_cols = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes']\n","    stats['numerical'] = {}\n","\n","    for col in num_cols:\n","        if col in df.columns:\n","            # Force numeric\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","            desc = df[col].describe()\n","            stats['numerical'][col] = {\n","                'mean': desc['mean'],\n","                'std': desc['std'],\n","                'min': desc['min'],\n","                'max': desc['max'],\n","                'median': desc['50%']\n","            }\n","        else:\n","            stats['numerical'][col] = None\n","\n","    # 3. Categorical Counts\n","    cat_cols = ['Proto', 'State', 'Dir']\n","    stats['categorical'] = {}\n","    for col in cat_cols:\n","        if col in df.columns:\n","            stats['categorical'][col] = df[col].value_counts().head(10).to_dict()\n","        else:\n","            stats['categorical'][col] = {}\n","\n","    # 4. Missing Values\n","    stats['missing_values'] = df.isnull().sum().to_dict()\n","\n","    return stats, df\n","\n","def generate_report(stats, df, output_dir, filename_base):\n","    \"\"\"Generates text report and plots.\"\"\"\n","\n","    # --- 1. Text Report ---\n","    report_path = os.path.join(output_dir, f\"{filename_base}_report.txt\")\n","    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(f\"BÁO CÁO PHÂN TÍCH DỮ LIỆU: {filename_base}\\n\")\n","        f.write(\"=\"*60 + \"\\n\\n\")\n","\n","        f.write(\"1. TỔNG QUAN\\n\")\n","        f.write(f\"   - Tổng số dòng: {len(df)}\\n\")\n","        f.write(f\"   - Số cột: {len(df.columns)}\\n\")\n","        f.write(\"\\n\")\n","\n","        f.write(\"2. PHÂN BỐ NHÃN (Lớp Mapped)\\n\")\n","        if stats['mapped_label_counts']:\n","            for label, count in stats['mapped_label_counts'].items():\n","                pct = stats['mapped_label_pct'].get(label, 0) * 100\n","                f.write(f\"   - {label}: {count} ({pct:.2f}%)\\n\")\n","        else:\n","            f.write(\"   (Không có dữ liệu nhãn)\\n\")\n","        f.write(\"\\n\")\n","\n","        f.write(\"3. TOP 20 NHÃN GỐC (Raw Labels)\\n\")\n","        if stats['raw_label_counts']:\n","            for label, count in stats['raw_label_counts'].items():\n","                f.write(f\"   - {label}: {count}\\n\")\n","        f.write(\"\\n\")\n","\n","        f.write(\"4. THỐNG KÊ SỐ HỌC (Numerical Stats)\\n\")\n","        for col, data in stats['numerical'].items():\n","            if data:\n","                f.write(f\"   * {col}:\\n\")\n","                f.write(f\"     - Trung bình (Mean): {data['mean']:.2f}\\n\")\n","                f.write(f\"     - Trung vị (Median): {data['median']:.2f}\\n\")\n","                f.write(f\"     - Lớn nhất (Max):    {data['max']:.2f}\\n\")\n","                f.write(f\"     - Độ lệch chuẩn (Std): {data['std']:.2f}\\n\")\n","            else:\n","                f.write(f\"   * {col}: Không tìm thấy\\n\")\n","        f.write(\"\\n\")\n","\n","        f.write(\"5. THÔNG TIN PHÂN LOẠI (Categorical Info)\\n\")\n","        for col, data in stats['categorical'].items():\n","            f.write(f\"   * Top 10 {col}:\\n\")\n","            for k, v in data.items():\n","                f.write(f\"     - {k}: {v}\\n\")\n","        f.write(\"\\n\")\n","\n","        f.write(\"6. GIÁ TRỊ THIẾU (Missing Values)\\n\")\n","        for col, count in stats['missing_values'].items():\n","            if count > 0:\n","                f.write(f\"   - {col}: {count}\\n\")\n","\n","    print(f\"Đã lưu báo cáo văn bản tại: {report_path}\")\n","\n","    # --- 2. Visualization ---\n","    image_path = os.path.join(output_dir, f\"{filename_base}_visuals.png\")\n","\n","    # Setup Figure: 2 Rows, 3 Columns\n","    fig = plt.figure(figsize=(20, 12), constrained_layout=True)\n","    gs = fig.add_gridspec(2, 3)\n","\n","    # A. Class Distribution (Mapped)\n","    ax1 = fig.add_subplot(gs[0, 0])\n","    if 'Mapped_Label' in df.columns:\n","        sns.countplot(x='Mapped_Label', data=df, ax=ax1, palette='viridis', hue='Mapped_Label', legend=False)\n","        ax1.set_title(\"Phân bố Lớp (Mapped Classes)\", fontsize=14)\n","        ax1.set_ylabel(\"Số lượng\")\n","        ax1.set_xlabel(\"Lớp\")\n","    else:\n","        ax1.text(0.5, 0.5, \"Không có dữ liệu nhãn\", ha='center')\n","\n","    # B. Numerical Distributions (Boxplots Log Scale)\n","    ax2 = fig.add_subplot(gs[0, 1])\n","    # Melting for easier plotting of multiple features\n","    num_cols = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes']\n","    # Filter valid columns\n","    valid_num = [c for c in num_cols if c in df.columns]\n","\n","    if valid_num:\n","        # We take a sample if data is too huge to plot quickly? No, boxplot is fast enough usually.\n","        # But we need log scale because Bytes can be huge.\n","        df_melt = df[valid_num].melt(var_name='Feature', value_name='Value')\n","        # Log transformation for display (handling 0)\n","        df_melt['LogValue'] = np.log1p(df_melt['Value'])\n","\n","        sns.boxplot(x='Feature', y='LogValue', data=df_melt, ax=ax2, hue='Feature', palette=\"Set2\")\n","        ax2.set_title(\"Phân bố Đặc trưng Số (Log Scale)\", fontsize=14)\n","        ax2.set_ylabel(\"Log(Giá trị + 1)\")\n","    else:\n","        ax2.text(0.5, 0.5, \"Không có dữ liệu số\", ha='center')\n","\n","    # C. Top Raw Labels (Horizontal Bar)\n","    ax3 = fig.add_subplot(gs[0, 2])\n","    if 'Label' in df.columns:\n","        top_labels = df['Label'].value_counts().head(10)\n","        sns.barplot(y=top_labels.index, x=top_labels.values, ax=ax3, palette=\"magma\", hue=top_labels.index, legend=False)\n","        ax3.set_title(\"Top 10 Nhãn Gốc (Raw Labels)\", fontsize=14)\n","        ax3.set_xlabel(\"Số lượng\")\n","    else:\n","        ax3.text(0.5, 0.5, \"Không có dữ liệu nhãn\", ha='center')\n","\n","    # D. Protocol Distribution\n","    ax4 = fig.add_subplot(gs[1, 0])\n","    if 'Proto' in df.columns:\n","        top_proto = df['Proto'].value_counts().head(10)\n","        sns.barplot(x=top_proto.index, y=top_proto.values, ax=ax4, palette=\"Blues_d\", hue=top_proto.index, legend=False)\n","        ax4.set_title(\"Top Giao thức (Protocol)\", fontsize=14)\n","        ax4.set_ylabel(\"Số lượng\")\n","    else:\n","        ax4.text(0.5, 0.5, \"Không có Proto\", ha='center')\n","\n","    # E. State Distribution\n","    ax5 = fig.add_subplot(gs[1, 1])\n","    if 'State' in df.columns:\n","        top_state = df['State'].value_counts().head(10)\n","        sns.barplot(x=top_state.index, y=top_state.values, ax=ax5, palette=\"Reds_d\", hue=top_state.index, legend=False)\n","        ax5.set_title(\"Top Trạng thái (State)\", fontsize=14)\n","        ax5.set_ylabel(\"Số lượng\")\n","        ax5.tick_params(axis='x', rotation=45)\n","    else:\n","        ax5.text(0.5, 0.5, \"Không có State\", ha='center')\n","\n","    # F. Feature by Class (e.g., TotBytes per Class)\n","    ax6 = fig.add_subplot(gs[1, 2])\n","    if 'Mapped_Label' in df.columns and 'TotBytes' in df.columns:\n","         # Use LogBytes\n","         df['LogTotBytes'] = np.log1p(df['TotBytes'])\n","         sns.boxplot(x='Mapped_Label', y='LogTotBytes', data=df, ax=ax6, palette=\"coolwarm\", hue='Mapped_Label', legend=False)\n","         ax6.set_title(\"TotBytes theo Lớp (Log Scale)\", fontsize=14)\n","         ax6.set_ylabel(\"Log(TotBytes)\")\n","    else:\n","        ax6.text(0.5, 0.5, \"Thiếu dữ liệu để vẽ biểu đồ\", ha='center')\n","\n","    # Save\n","    plt.suptitle(f\"Dashboard Phân Tích: {filename_base}\", fontsize=20)\n","    plt.savefig(image_path)\n","    plt.close()\n","\n","    print(f\"Đã lưu biểu đồ tại: {image_path}\")\n","\n","\n","def analyze_csv_main():\n","    parser = argparse.ArgumentParser(description=\"Script phân tích file CSV CTU-13\")\n","    parser.add_argument(\"--file\", type=str, required=True, help=\"Đường dẫn đến file .csv\")\n","    parser.add_argument(\"--outdir\", type=str, default=\"analysis_reports\", help=\"Thư mục lưu kết quả\")\n","\n","    args = parser.parse_args()\n","\n","    # Create output dir\n","    os.makedirs(args.outdir, exist_ok=True)\n","\n","    # Filename base\n","    base_name = os.path.splitext(os.path.basename(args.file))[0]\n","    # To avoid overwriting if same filename exists in different folders, maybe prepend parent folder?\n","    # e.g. 10_10.csv\n","    parent_dir = os.path.basename(os.path.dirname(args.file))\n","    if parent_dir.isdigit():\n","        base_name = f\"{parent_dir}_{base_name}\"\n","\n","    try:\n","        df = load_data(args.file)\n","        stats, df_processed = analyze_data(df)\n","        generate_report(stats, df_processed, args.outdir, base_name)\n","        print(\"Hoàn tất!\")\n","    except Exception as e:\n","        print(f\"LỖI: {e}\")\n","\n","# if __name__ == \"__main__\": # Block disabled for notebook import\n","#     analyze_csv_main()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"AIh_or7Z1muJ","executionInfo":{"status":"ok","timestamp":1766295923501,"user_tz":-420,"elapsed":7,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"id":"dHzvxU62QLz8","executionInfo":{"status":"ok","timestamp":1766295923539,"user_tz":-420,"elapsed":31,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: train.py (UPDATED WITH FOCAL LOSS + WEIGHTED SAMPLER)\n","# ==========================================\n","import os\n","import gc\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.utils import class_weight\n","from collections import Counter\n","from imblearn.over_sampling import SMOTE\n","from tqdm.auto import tqdm\n","# from loss import FocalLoss  # Commented out for notebook compatibility\n","try:\n","    from torchinfo import summary\n","except ImportError:\n","    summary = None\n","\n","# # from config import (  # Commented out for notebook compatibility\n","#     WORKING_DIR, BATCH_SIZE, N_WORKERS, N_EPOCHS, LEARNING_RATE,\n","#     IMAGE_SIZE, TRAIN_SCENARIOS, TEST_SCENARIOS,\n","#     CLASS_TO_IDX, device\n","# )\n","# # from utils import (  # Commented out for notebook compatibility\n","#     create_directory, download_file, check_csv_in_folder,\n","#     rename, get_csv_paths, plot_and_save_loss\n","# )\n","# # from preprocessing_utils import (  # Commented out for notebook compatibility\n","#     quick_classify, calculate_global_frequencies, process_batch_fast_v2,\n","#     save_global_stats, save_scaler\n","# )\n","# from model import BotnetClassifier  # Commented out for notebook compatibility\n","# from data_loader import FastBotnetDataset, load_data_from_csvs  # Commented out for notebook compatibility\n","\n","def train_main(ispart=True):\n","    print(f\"Working directory: {WORKING_DIR}\")\n","    print(\"=\"*60)\n","    print(\"SYSTEM RESOURCES\")\n","    print(\"=\"*60)\n","\n","    if device.type == 'cuda':\n","        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n","\n","    print(f\"\\n\" + \"=\"*60)\n","    print(\"CONFIGURATION\")\n","    print(\"=\"*60)\n","    print(f\"  Batch Size: {BATCH_SIZE}\")\n","    print(f\"  Epochs: {N_EPOCHS}\")\n","    print(f\"  Learning Rate: {LEARNING_RATE}\")\n","    print(f\"  DataLoader Workers: {N_WORKERS}\")\n","    print(\"=\"*60)\n","\n","    # =============================================================================\n","    # 1. DOWNLOAD DATASET\n","    # =============================================================================\n","    main_dir = os.path.join(WORKING_DIR, 'CTU-13-Dataset')\n","    create_directory(main_dir)\n","\n","    for i in range(1, 14):\n","        create_directory(os.path.join(main_dir, str(i)))\n","\n","    datasets = [\n","        (8, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-49/detailed-bidirectional-flow-labels/capture20110816-3.binetflow'),\n","        (10, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-51/detailed-bidirectional-flow-labels/capture20110818.binetflow'),\n","    ]\n","\n","    print(f\"\\nStarting download for {len(datasets)} datasets...\")\n","    for idx, url in datasets:\n","        filename = url.split('/')[-1]\n","        destination = os.path.join(main_dir, str(idx), filename)\n","        folder_path = os.path.join(main_dir, str(idx))\n","\n","        print(f\"\\n[{idx}/13] Dataset {idx}:\")\n","        if check_csv_in_folder(folder_path):\n","            print(f\"  [SKIP] CSV already exists.\")\n","            continue\n","        download_file(url, destination)\n","    print(\"\\nDownload complete!\")\n","\n","    # =============================================================================\n","    # 2. CONVERT BINETFLOW TO CSV\n","    # =============================================================================\n","    listDir = os.listdir(main_dir)\n","    listCSV = []\n","    print(\"Checking/Converting files...\")\n","\n","    for subDir in sorted(listDir, key=lambda x: int(x) if x.isdigit() else 0):\n","        path_subDir = os.path.join(main_dir, subDir)\n","        if not os.path.isdir(path_subDir): continue\n","\n","        # Check if CSV exists\n","        csv_files = [f for f in os.listdir(path_subDir) if f.endswith('.csv')]\n","        if csv_files:\n","            listCSV.append(os.path.join(path_subDir, csv_files[0]))\n","            continue\n","\n","        # If no CSV, look for binetflow to rename\n","        binetflow_files = [f for f in os.listdir(path_subDir) if 'binetflow' in f]\n","        if binetflow_files:\n","            binetflow_file = os.path.join(path_subDir, binetflow_files[0])\n","            new_name = subDir + '.csv'\n","            rename(binetflow_file, new_name)\n","            listCSV.append(os.path.join(path_subDir, new_name))\n","            print(f\"  Converted {binetflow_files[0]} -> {new_name}\")\n","\n","    print(f\"\\nFound {len(listCSV)} CSV files:\")\n","    for csv in listCSV:\n","        print(f\"  {csv}\")\n","\n","    # =============================================================================\n","    # 3. IDENTIFY SCENARIOS\n","    # =============================================================================\n","    train_csvs = get_csv_paths(main_dir, TRAIN_SCENARIOS)\n","    test_csvs = get_csv_paths(main_dir, TEST_SCENARIOS)\n","\n","    print(f\"Found {len(train_csvs)} training files.\")\n","    print(f\"Found {len(test_csvs)} testing files.\")\n","\n","    if not train_csvs:\n","        print(\"No training files found! Exiting.\")\n","        return\n","\n","    # =============================================================================\n","    # 4. PRE-COMPUTE GLOBAL STATISTICS\n","    # =============================================================================\n","    print(\"=\"*70)\n","    print(\"PRE-COMPUTING GLOBAL STATISTICS\")\n","    print(\"=\"*70)\n","\n","    target_csvs = train_csvs\n","\n","    # --- Step 1: Calculate Global Frequencies ---\n","    print(\"\\n[1/4] Calculating global IP/Port frequencies...\")\n","    freq_dicts = calculate_global_frequencies(target_csvs)\n","\n","    # --- Step 2: Detect Column Schema ---\n","    print(\"\\n[2/3] Detecting column schema...\")\n","    expected_columns = None\n","    cols_samples = []\n","\n","    for csv_file in target_csvs[:5]:\n","        try:\n","            chunk = pd.read_csv(csv_file, nrows=5000, low_memory=False)\n","            X_s, y_s, cols_s = process_batch_fast_v2(chunk, freq_dicts, expected_columns=None)\n","            if cols_s:\n","                cols_samples.extend(cols_s)\n","        except Exception:\n","            continue\n","\n","    if cols_samples:\n","        expected_columns = list(dict.fromkeys(cols_samples))  # Preserve order, remove duplicates\n","        print(f\"  Detected {len(expected_columns)} feature columns\")\n","    else:\n","        print(\"  WARNING: Could not detect column schema!\")\n","        return\n","\n","    # --- Step 3: Chuẩn bị global_stats ---\n","    n_features = len(expected_columns)\n","\n","    global_stats = {\n","        'freq_dicts': freq_dicts,\n","        'expected_columns': expected_columns,\n","        'n_features': n_features\n","    }\n","\n","    # =============================================================================\n","    # 5. LOAD DATA INTO RAM\n","    # =============================================================================\n","    scaler = RobustScaler()\n","\n","    print(\"\\nLoading TRAINING Data...\")\n","    X_train, y_train = load_data_from_csvs(train_csvs, global_stats, desc=\"Train Data\", is_train=True, scaler=scaler)\n","\n","    # Optionally use only a part of the training set while keeping class ratio\n","    if ispart:\n","        subset_fraction = 0.1  # dùng 10% dữ liệu train, vẫn giữ đúng tỉ lệ class\n","        print(f\"\\n[ISPART] Using only {subset_fraction*100:.1f}% of training data with stratified sampling...\")\n","        X_train, _, y_train, _ = train_test_split(\n","            X_train,\n","            y_train,\n","            train_size=subset_fraction,\n","            stratify=y_train,\n","            random_state=42\n","        )\n","        print(f\"New Train shape after ISPART: {X_train.shape}\")\n","\n","    # Save statistics and scaler after processing training data\n","    print(\"\\nSaving Global Statistics and Scaler...\")\n","    save_global_stats(global_stats, filepath=os.path.join(WORKING_DIR, 'global_stats.pkl'))\n","    save_scaler(scaler, filepath=os.path.join(WORKING_DIR, 'scaler.pkl'))\n","\n","    print(\"\\nLoading TESTING Data...\")\n","    X_test, y_test = load_data_from_csvs(test_csvs, global_stats, desc=\"Test Data\", is_train=False, scaler=scaler)\n","\n","    print(f\"\\nTrain shape: {X_train.shape}\")\n","    print(f\"Test shape:  {X_test.shape}\")\n","\n","    # =============================================================================\n","    # 6. TÍNH CLASS WEIGHTS TRƯỚC KHI ÁP DỤNG SMOTE\n","    # =============================================================================\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"COMPUTING CLASS WEIGHTS (BEFORE SMOTE)\")\n","    print(\"=\"*70)\n","\n","    print(f\"Phân phối class gốc: {Counter(y_train)}\")\n","\n","    # Tính class weights từ dữ liệu GỐC (trước SMOTE) để giữ thông tin imbalance\n","    class_weights_array = class_weight.compute_class_weight(\n","        'balanced',\n","        classes=np.unique(y_train),\n","        y=y_train\n","    )\n","    class_weights_tensor = torch.FloatTensor(class_weights_array).to(device)\n","\n","    print(f\"Class weights: {dict(zip(np.unique(y_train), class_weights_array))}\")\n","    print(\"=\"*70)\n","\n","    # =============================================================================\n","    # 7. APPLY SMOTE (CHỈ TRÊN TRAIN SET)\n","    # =============================================================================\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"APPLYING SMOTE TO BALANCE TRAINING DATA\")\n","    print(\"=\"*70)\n","\n","    print(f\"\\nPhân phối class TRƯỚC SMOTE: {Counter(y_train)}\")\n","\n","    # Áp dụng SMOTE để tăng số lượng mẫu Botnet\n","    # CHỈ áp dụng trên tập train, KHÔNG đụng vào test/val\n","    smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy='auto')\n","    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n","\n","    print(f\"Phân phối class SAU SMOTE: {Counter(y_train_res)}\")\n","    print(f\"Train shape sau SMOTE: {X_train_res.shape}\")\n","    print(\"=\"*70)\n","\n","    # =============================================================================\n","    # 8. MODEL SET UP WITH FOCAL LOSS + WEIGHTED SAMPLER\n","    # =============================================================================\n","\n","    # Validation Split (sau khi đã áp dụng SMOTE)\n","    X_train_final, X_val, y_train_final, y_val = train_test_split(\n","        X_train_res, y_train_res, test_size=0.2, random_state=42, stratify=y_train_res\n","    )\n","\n","    train_ds = FastBotnetDataset(X_train_final, y_train_final)\n","\n","    # CRITICAL FIX: Tạo WeightedRandomSampler để oversample minority classes\n","    print(\"\\n[CRITICAL FIX] Creating WeightedRandomSampler...\")\n","    sample_weights = []\n","    for label in y_train_final:\n","        # Sử dụng class weights từ phân phối GỐC (trước SMOTE)\n","        sample_weights.append(class_weights_array[label])\n","\n","    sampler = WeightedRandomSampler(\n","        weights=sample_weights,\n","        num_samples=len(sample_weights),\n","        replacement=True\n","    )\n","\n","    print(f\"  Created sampler with {len(sample_weights)} samples\")\n","\n","    # CRITICAL: Dùng sampler thay vì shuffle=True\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=BATCH_SIZE,\n","        sampler=sampler,  # KHÔNG dùng shuffle khi có sampler!\n","        num_workers=N_WORKERS\n","    )\n","\n","    valid_ds = FastBotnetDataset(X_val, y_val)\n","    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=N_WORKERS)\n","\n","    test_ds = FastBotnetDataset(X_test, y_test)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=N_WORKERS)\n","\n","    # Model\n","    model = BotnetClassifier(base_model=None, n_features=n_features, image_size=IMAGE_SIZE, n_classes=len(CLASS_TO_IDX))\n","    model = model.to(device)\n","\n","    if summary:\n","        # Input: ảnh 1x32x32\n","        summary(model, input_size=(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE))\n","    else:\n","        print(model)\n","\n","    # CRITICAL FIX: Dùng Focal Loss + class weights\n","    print(\"\\n[CRITICAL FIX] Using Focal Loss with class weights...\")\n","    criterion = FocalLoss(weight=class_weights_tensor, gamma=2.0)\n","    print(f\"  Focal Loss gamma=2.0, class_weights={class_weights_tensor}\")\n","\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    # Thêm scheduler để giảm learning rate dần\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N_EPOCHS)\n","    print(f\"  Using CosineAnnealingLR scheduler\")\n","\n","    # =============================================================================\n","    # 9. TRAINING LOOP\n","    # =============================================================================\n","    train_losses = []\n","    valid_losses = []\n","\n","    best_val_loss = float('inf')\n","\n","    model_save_path = os.path.join(WORKING_DIR, f'best_model{IMAGE_SIZE}.pth')\n","    loss_plot_path = os.path.join(WORKING_DIR, f'training_history_loss_{N_EPOCHS}.png')\n","\n","    print(f\"\\nStarting training on {device}...\")\n","    print(f\"Models and plots will be saved to: {WORKING_DIR}\")\n","    print(\"=\"*70)\n","\n","    for epoch in range(1, N_EPOCHS + 1):\n","        model.train()\n","        running_loss = 0.0\n","\n","        # Training\n","        for X_batch, y_batch in tqdm(train_loader, desc=f\"Ep {epoch}/{N_EPOCHS} [Train]\", leave=False):\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(X_batch)\n","            loss = criterion(outputs, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * X_batch.size(0)\n","\n","        epoch_train_loss = running_loss / len(train_loader.dataset)\n","        train_losses.append(epoch_train_loss)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for X_batch, y_batch in tqdm(valid_loader, desc=f\"Ep {epoch}/{N_EPOCHS} [Val]\", leave=False):\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                outputs = model(X_batch)\n","                loss = criterion(outputs, y_batch)\n","                val_loss += loss.item() * X_batch.size(0)\n","\n","        epoch_val_loss = val_loss / len(valid_loader.dataset)\n","        valid_losses.append(epoch_val_loss)\n","\n","        # Step scheduler\n","        scheduler.step()\n","        current_lr = scheduler.get_last_lr()[0]\n","\n","        print(f\"Epoch {epoch}: Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | LR: {current_lr:.6f}\")\n","\n","        # Checkpoint\n","        if epoch_val_loss < best_val_loss:\n","            print(f\"  Val loss decreased ({best_val_loss:.4f} -> {epoch_val_loss:.4f}). Saving model to {model_save_path}...\")\n","            torch.save(model.state_dict(), model_save_path)\n","            best_val_loss = epoch_val_loss\n","\n","        # Save loss plot mỗi epoch\n","        if epoch % 5 == 0 or epoch == N_EPOCHS:\n","            plot_and_save_loss(train_losses, valid_losses, loss_plot_path)\n","\n","    # =============================================================================\n","    # 10. RESULTS\n","    # =============================================================================\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"TRAINING COMPLETE\")\n","    print(\"=\"*70)\n","    print(f\"Best validation loss: {best_val_loss:.4f}\")\n","    print(f\"Model saved to: {model_save_path}\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"G2SOtoSB2DEx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766295925347,"user_tz":-420,"elapsed":1799,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}},"outputId":"1d062548-8cc9-4e55-f3b1-8dc474630d2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":true,"id":"Zqwem2OyQLz9","executionInfo":{"status":"ok","timestamp":1766295925512,"user_tz":-420,"elapsed":163,"user":{"displayName":"Nhật Nguyễn","userId":"03573233896341887589"}}},"outputs":[],"source":["# ==========================================\n","# CONTENT FROM: evaluate.py\n","# ==========================================\n","import os\n","import argparse\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.preprocessing import RobustScaler\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","import gc\n","\n","# Lưu ý:\n","# - Các biến cấu hình (WORKING_DIR, BATCH_SIZE, TRAIN_SCENARIOS, TEST_SCENARIOS,\n","#   CLASS_TO_IDX, IDX_TO_CLASS, device) đã được định nghĩa trong cell \"config.py\" ở trên.\n","# - Các hàm tiện ích (get_csv_paths, create_directory, quick_classify, ...)\n","#   và class (BotnetClassifier, FastBotnetDataset, load_data_from_csvs, ...)\n","#   cũng đã được định nghĩa ở các cell trước (utils.py, preprocessing_utils.py, model.py, data_loader.py).\n","# => Không cần import lại ở đây để tránh trùng lặp / lỗi cú pháp.\n","\n","def compute_stats_from_train(main_dir):\n","    \"\"\"\n","    Recomputes global stats and scaler from training scenarios.\n","    \"\"\"\n","    print(\"Recomputing statistics from training data...\")\n","    train_csvs = get_csv_paths(main_dir, TRAIN_SCENARIOS)\n","\n","    if not train_csvs:\n","        print(\"No training files found! Please check TRAIN_SCENARIOS and dataset.\")\n","        return None, None\n","\n","    # --- Step 1: Calculate Global Frequencies ---\n","    print(\"\\n[1/2] Calculating global IP/Port frequencies...\")\n","    freq_dicts = calculate_global_frequencies(train_csvs)\n","\n","    # --- Step 2: Detect Column Schema ---\n","    print(\"\\n[2/2] Detecting column schema...\")\n","    expected_columns = None\n","    cols_samples = []\n","\n","    for csv_file in train_csvs[:5]:\n","        try:\n","            chunk = pd.read_csv(csv_file, nrows=5000, low_memory=False)\n","            X_s, y_s, cols_s = process_batch_fast_v2(chunk, freq_dicts, expected_columns=None)\n","            if cols_s:\n","                cols_samples.extend(cols_s)\n","        except Exception:\n","            continue\n","\n","    if cols_samples:\n","        expected_columns = list(dict.fromkeys(cols_samples))\n","        print(f\"  Detected {len(expected_columns)} feature columns\")\n","    else:\n","        print(\"  WARNING: Could not detect column schema!\")\n","        return None, None\n","\n","    global_stats = {\n","        'freq_dicts': freq_dicts,\n","        'expected_columns': expected_columns,\n","        'n_features': len(expected_columns)\n","    }\n","\n","    # --- Step 4: Fit Scaler ---\n","    print(\"\\nFitting Scaler on Training Data...\")\n","    scaler = RobustScaler()\n","    # We need to load training data to fit scaler.\n","    # This might be heavy, but necessary for correct evaluation if stats are missing.\n","    X_train, _ = load_data_from_csvs(train_csvs, global_stats, desc=\"Fitting Scaler\", is_train=True, scaler=scaler)\n","\n","    # Save for future\n","    save_global_stats(global_stats, filepath=os.path.join(WORKING_DIR, 'global_stats.pkl'))\n","    save_scaler(scaler, filepath=os.path.join(WORKING_DIR, 'scaler.pkl'))\n","\n","    # Clean up RAM\n","    del X_train\n","    gc.collect()\n","\n","    return global_stats, scaler\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X_batch, y_batch in tqdm(test_loader, desc=\"Evaluating\"):\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","            outputs = model(X_batch)\n","            _, preds = torch.max(outputs, 1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(y_batch.cpu().numpy())\n","\n","    return np.array(all_labels), np.array(all_preds)\n","\n","def plot_confusion_matrix_vietnamese(cm, classes, save_path):\n","    plt.figure(figsize=(10, 8))\n","\n","    # Map class names to Vietnamese if desired, or keep English but title in VN\n","    # Common mappings: Normal -> Bình thường, Botnet -> Botnet, C&C -> C&C\n","    # Let's keep class names technical but title/labels in VN.\n","\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.title('Ma trận nhầm lẫn (Confusion Matrix)', fontsize=16)\n","    plt.ylabel('Nhãn thực tế (True Label)', fontsize=12)\n","    plt.xlabel('Nhãn dự đoán (Predicted Label)', fontsize=12)\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def evaluate_main():\n","    parser = argparse.ArgumentParser(description=\"Evaluate Botnet Detection Model\")\n","    parser.add_argument('--recompute-stats', action='store_true', help=\"Recompute global stats and scaler from training data instead of loading.\")\n","    args = parser.parse_args()\n","\n","    print(f\"Device: {device}\")\n","    print(f\"Working Directory: {WORKING_DIR}\")\n","\n","    # Check/Download Data (Reuse logic if needed, assuming data exists in CTU-13-Dataset)\n","    main_dir = os.path.join(WORKING_DIR, 'CTU-13-Dataset')\n","    if not os.path.exists(main_dir):\n","        print(\"Dataset directory not found. Please run train.py to download data first.\")\n","        return\n","\n","    # 1. Load Stats and Scaler\n","    stats_path = os.path.join(WORKING_DIR, 'global_stats.pkl')\n","    scaler_path = os.path.join(WORKING_DIR, 'scaler.pkl')\n","\n","    if args.recompute_stats or not os.path.exists(stats_path) or not os.path.exists(scaler_path):\n","        global_stats, scaler = compute_stats_from_train(main_dir)\n","        if global_stats is None:\n","            return\n","    else:\n","        print(\"Loading existing statistics and scaler...\")\n","        global_stats = load_global_stats(stats_path)\n","        scaler = load_scaler(scaler_path)\n","\n","    if global_stats is None:\n","        print(\"Failed to obtain global statistics.\")\n","        return\n","\n","    # 2. Load Test Data\n","    test_csvs = get_csv_paths(main_dir, TEST_SCENARIOS)\n","    print(f\"Found {len(test_csvs)} testing files for evaluation.\")\n","\n","    # Load data\n","    X_test, y_test = load_data_from_csvs(test_csvs, global_stats, desc=\"Loading Test Data\", is_train=False, scaler=scaler)\n","\n","    print(f\"Test Data Shape: {X_test.shape}\")\n","\n","    # 3. Load Model\n","    model_path = os.path.join(WORKING_DIR, f'best_model{IMAGE_SIZE}.pth')\n","    if not os.path.exists(model_path):\n","        print(f\"Model file {model_path} not found!\")\n","        return\n","\n","    # n_features được giữ cho tương thích API nhưng không dùng trong CNN ảnh\n","    model = BotnetClassifier(base_model=None, n_features=global_stats['n_features'], image_size=IMAGE_SIZE, n_classes=len(CLASS_TO_IDX))\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model = model.to(device)\n","    print(\"Model loaded successfully.\")\n","\n","    # 4. Evaluate\n","    test_ds = FastBotnetDataset(X_test, y_test)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=N_WORKERS) # 0 workers for safety\n","\n","    y_true, y_pred = evaluate_model(model, test_loader, device)\n","\n","    # Quick sanity check: đếm số mẫu theo lớp trong ground-truth và dự đoán\n","    label_indices = list(range(len(CLASS_TO_IDX)))\n","    true_counts = {IDX_TO_CLASS[i]: int((y_true == i).sum()) for i in label_indices}\n","    pred_counts = {IDX_TO_CLASS[i]: int((y_pred == i).sum()) for i in label_indices}\n","    print(\"\\nPhân bố nhãn (ground-truth):\", true_counts)\n","    print(\"Phân bố nhãn (dự đoán):     \", pred_counts)\n","\n","    # 5. Metrics & Reporting (Vietnamese)\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n","    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n","    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n","\n","    # Use a fixed label order based on CLASS_TO_IDX so that\n","    # confusion matrix and classification report are consistent\n","    label_indices = list(range(len(CLASS_TO_IDX)))\n","    cm = confusion_matrix(y_true, y_pred, labels=label_indices)\n","\n","    # Prepare output directory\n","    metrics_dir = os.path.join(WORKING_DIR, 'metrics')\n","    create_directory(metrics_dir)\n","\n","    report_path = os.path.join(metrics_dir, f'evaluation_report{IMAGE_SIZE}.txt')\n","    cm_path = os.path.join(metrics_dir, f'confusion_matrix{IMAGE_SIZE}.png')\n","\n","    report_content = [\n","        \"BÁO CÁO ĐÁNH GIÁ MÔ HÌNH (MODEL EVALUATION REPORT)\",\n","        \"=\"*50,\n","        f\"Kịch bản kiểm thử (Test Scenarios): {TEST_SCENARIOS}\",\n","        f\"Tổng số mẫu (Total Samples): {len(y_true)}\",\n","        \"-\"*50,\n","        f\"Độ chính xác (Accuracy): {accuracy:.4f}\",\n","        f\"Độ chính xác (Precision - Weighted): {precision:.4f}\",\n","        f\"Độ nhạy (Recall - Weighted): {recall:.4f}\",\n","        f\"Điểm F1 (F1-Score - Weighted): {f1:.4f}\",\n","        \"=\"*50,\n","        \"Chi tiết theo lớp (Class-wise Details):\"\n","    ]\n","\n","    # Class-wise metrics\n","    # Re-map indices to class names (must match label_indices order)\n","    classes = [IDX_TO_CLASS[i] for i in label_indices]\n","\n","    # Calculate per-class precision/recall/f1.\n","    # Explicitly pass labels so that number of labels matches target_names,\n","    # even if some classes do not appear in y_true/y_pred.\n","    from sklearn.metrics import classification_report\n","    cls_report = classification_report(\n","        y_true,\n","        y_pred,\n","        labels=label_indices,\n","        target_names=classes,\n","        output_dict=True,\n","        zero_division=0,\n","    )\n","\n","    for cls in classes:\n","        metrics = cls_report.get(cls, {})\n","        report_content.append(f\"\\nLớp: {cls}\")\n","        report_content.append(f\"  Precision: {metrics.get('precision', 0):.4f}\")\n","        report_content.append(f\"  Recall:    {metrics.get('recall', 0):.4f}\")\n","        report_content.append(f\"  F1-Score:  {metrics.get('f1-score', 0):.4f}\")\n","\n","\n","    # Export metrics to CSV\n","    import csv\n","    csv_path = os.path.join(metrics_dir, f'evaluation_metrics{IMAGE_SIZE}.csv')\n","\n","    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow(['Metric', 'Value'])\n","        writer.writerow(['Accuracy', f'{accuracy:.4f}'])\n","        writer.writerow(['Precision', f'{precision:.4f}'])\n","        writer.writerow(['Recall', f'{recall:.4f}'])\n","        writer.writerow(['F1-Score', f'{f1:.4f}'])\n","        writer.writerow(['Image Size', IMAGE_SIZE])\n","        writer.writerow(['Model Path', model_path])\n","\n","    print(f\"Metrics exported to CSV: {csv_path}\")\n","\n","    with open(report_path, 'w', encoding='utf-8') as f:\n","        f.write('\\n'.join(report_content))\n","\n","    print(f\"\\nReport saved to {report_path}\")\n","    print('\\n'.join(report_content))\n","\n","    # Plot Confusion Matrix\n","    plot_confusion_matrix_vietnamese(cm, classes, cm_path)\n","    print(f\"Confusion Matrix saved to {cm_path}\")"]},{"cell_type":"markdown","metadata":{"id":"ePC638Mg2HFw"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b11a34eee2b845ae9f0d37a1e9c75a3b","b25b49e8b9f843cba7a18d7010ba3a50","717e8a069a40436889d9efa521673020","59274a4ef460413c9646a08f516ed248","4d5ac279ec7945b1b5e51f6168e014a9","144ecfbf1b8f40b0acfd7f3e7713c605","575796e7c3074aa1b0acdd92f573abdb","56181df4c3024264ad6f7f2e730125a3","41109f101fc8415f8a79355a379c73c4","103e185e82f24b1389e68cc60c1ba65a","6e858217d6eb41e8b837118eeb60dbd9","d440e16c1937470aa822d72cf740768b","c458bcc3367b4cd0bbd5e8b0ce41d2e2","e800f641ccfe40948c4d37620ff760d6","a36f8ae88034402db93ee60551445e26","a29423e5c2894671a5c7dec5b2283c88","4bfb42d31721400297bb2785cee9bf50","4684c539d072468db29b7b202fc0c4e2","20db89e6752c4c7bba489630a2b40b6f","58c7f88fc4e043c6a0c402409e64ed3b","96f3f6fb5bcd47579c6532a412e02654","77597d57dfd143f7a455f25118e332b0","9722e273b543425cb6d5d1c5236155f3","ae1de79fa6004ce59ba122ad72bd4700","c01ffcd7a4474f5e9b5d22cf1b58d9e3","6b0d350221f04bdcaadd11d6ae0a6ac4","09ccd61d264b4ee3b710dac9295a9da3","8c7a0d0d64dc41f98441bba2ff301e90","5b9bf4243c51447cb77346345b3007b6","f9cba03f8d2e4ffda48ba87c6f499406","a1a2794ccc33401c8ec8b6858f0fb1cf","12ac19f21cfa4e93a18876299ce85866","db1e79e1ae3e4f3fb30af6c0b36edaf3","8d9454fbbad243d78c87b247f153fdc2","a3b67f14a0d54f6e8e7f6a10352afb96","63a4f41b37394c1580282310b603c449","16091d045a5b4bb38c6df808c480395c","44f5725ba3c84171919a37f0c5e47373","02ab40a3d3dc4af5a0ecf404f61ff645","50522786468e492a93b453da63ca3074","c26d2f9f09d54e9494ffaea749ce7c85","93bc464bd78e4158b842b7d51b2b4879","50251a4538294248875c362f7f33e18b","ea1dd1629cfe47c5b7c6f7bad5ed3adb"]},"id":"qH8tMop6QLz9","outputId":"e1e7b9d6-ea19-42ee-d92a-0c50b841d521"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Training...\n","Working directory: /content/drive/MyDrive/botnet\n","============================================================\n","SYSTEM RESOURCES\n","============================================================\n","  GPU: Tesla T4\n","\n","============================================================\n","CONFIGURATION\n","============================================================\n","  Batch Size: 512\n","  Epochs: 20\n","  Learning Rate: 0.001\n","  DataLoader Workers: 0\n","============================================================\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/1\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/2\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/3\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/4\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/5\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/6\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/7\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/8\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/9\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/10\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/11\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/12\n","  Created/Checked: /content/drive/MyDrive/botnet/CTU-13-Dataset/13\n","\n","Starting download for 2 datasets...\n","\n","[8/13] Dataset 8:\n","  [SKIP] CSV already exists.\n","\n","[10/13] Dataset 10:\n","  [SKIP] CSV already exists.\n","\n","Download complete!\n","Checking/Converting files...\n","\n","Found 3 CSV files:\n","  /content/drive/MyDrive/botnet/CTU-13-Dataset/1/1.csv\n","  /content/drive/MyDrive/botnet/CTU-13-Dataset/8/8.csv\n","  /content/drive/MyDrive/botnet/CTU-13-Dataset/10/10.csv\n","Found 1 training files.\n","Found 1 testing files.\n","======================================================================\n","PRE-COMPUTING GLOBAL STATISTICS\n","======================================================================\n","\n","[1/4] Calculating global IP/Port frequencies...\n","Scanning CSVs for frequencies...\n"]},{"output_type":"display_data","data":{"text/plain":["Global Freqs:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b11a34eee2b845ae9f0d37a1e9c75a3b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","[2/3] Detecting column schema...\n","  Detected 107 feature columns\n","\n","Loading TRAINING Data...\n"]},{"output_type":"display_data","data":{"text/plain":["Train Data:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d440e16c1937470aa822d72cf740768b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Saving Global Statistics and Scaler...\n","Global stats saved to /content/drive/MyDrive/botnet/global_stats.pkl\n","Scaler saved to /content/drive/MyDrive/botnet/scaler.pkl\n","\n","Loading TESTING Data...\n"]},{"output_type":"display_data","data":{"text/plain":["Test Data:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9722e273b543425cb6d5d1c5236155f3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Train shape: (2954230, 107)\n","Test shape:  (1309791, 107)\n","\n","======================================================================\n","COMPUTING CLASS WEIGHTS (BEFORE SMOTE)\n","======================================================================\n","Phân phối class gốc: Counter({np.int64(1): 2948103, np.int64(0): 6127})\n","Class weights: {np.int64(0): np.float64(241.0829117023013), np.int64(1): np.float64(0.501039142797928)}\n","======================================================================\n","\n","======================================================================\n","APPLYING SMOTE TO BALANCE TRAINING DATA\n","======================================================================\n","\n","Phân phối class TRƯỚC SMOTE: Counter({np.int64(1): 2948103, np.int64(0): 6127})\n","Phân phối class SAU SMOTE: Counter({np.int64(1): 2948103, np.int64(0): 2948103})\n","Train shape sau SMOTE: (5896206, 107)\n","======================================================================\n","Computing global statistics for image normalization...\n","  Global stats: p1=-3.2149, p99=1.9068, mean=-0.0318, std=0.5045\n","\n","[CRITICAL FIX] Creating WeightedRandomSampler...\n","  Created sampler with 4716964 samples\n","Computing global statistics for image normalization...\n","  Global stats: p1=-3.2149, p99=1.8762, mean=-0.0317, std=0.5017\n","Computing global statistics for image normalization...\n","  Global stats: p1=-2.3845, p99=3.6134, mean=0.0284, std=0.5369\n","BotnetClassifier(\n","  (model): BotnetImageCNN(\n","    (features): Sequential(\n","      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (7): Dropout2d(p=0.1, inplace=False)\n","      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (10): ReLU(inplace=True)\n","      (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (13): ReLU(inplace=True)\n","      (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (15): Dropout2d(p=0.1, inplace=False)\n","      (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (18): ReLU(inplace=True)\n","      (19): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (21): ReLU(inplace=True)\n","      (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (23): Dropout2d(p=0.2, inplace=False)\n","      (24): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (26): ReLU(inplace=True)\n","      (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (29): ReLU(inplace=True)\n","      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (31): Dropout2d(p=0.2, inplace=False)\n","      (32): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (34): ReLU(inplace=True)\n","      (35): AdaptiveAvgPool2d(output_size=1)\n","    )\n","    (classifier): Sequential(\n","      (0): Dropout(p=0.5, inplace=False)\n","      (1): Linear(in_features=512, out_features=128, bias=True)\n","      (2): ReLU(inplace=True)\n","      (3): Dropout(p=0.3, inplace=False)\n","      (4): Linear(in_features=128, out_features=2, bias=True)\n","    )\n","  )\n",")\n","\n","[CRITICAL FIX] Using Focal Loss with class weights...\n","  Focal Loss gamma=2.0, class_weights=tensor([241.0829,   0.5010], device='cuda:0')\n","  Using CosineAnnealingLR scheduler\n","\n","Starting training on cuda...\n","Models and plots will be saved to: /content/drive/MyDrive/botnet\n","======================================================================\n"]},{"output_type":"display_data","data":{"text/plain":["Ep 1/20 [Train]:   0%|          | 0/9213 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d9454fbbad243d78c87b247f153fdc2"}},"metadata":{}}],"source":["# ==========================================\n","# EXECUTION: TRAIN\n","# ==========================================\n","if __name__ == '__main__':\n","    print('Starting Training...')\n","    # Set ispart=False for full training\n","    train_main(ispart=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tZm5cG2QLz9"},"outputs":[],"source":["# ==========================================\n","# EXECUTION: EVALUATE\n","# ==========================================\n","import sys\n","# Simulate command line arguments. Use empty list for defaults.\n","# To recompute stats: sys.argv = ['evaluate.py', '--recompute-stats']\n","sys.argv = ['evaluate.py']\n","\n","if __name__ == '__main__':\n","    print('Starting Evaluation...')\n","    evaluate_main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eF7K8raFQLz9"},"outputs":[],"source":["# ==========================================\n","# EXECUTION: ANALYZE CSV (Example)\n","# ==========================================\n","# sys.argv = ['analyze_csv.py', '--file', './CTU-13-Dataset/1/capture20110810.csv']\n","# analyze_csv_main()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.14"},"vincent":{"sessionId":"84a74bc4065ec77092161caf_2025-12-16T10-23-44-772Z"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b11a34eee2b845ae9f0d37a1e9c75a3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b25b49e8b9f843cba7a18d7010ba3a50","IPY_MODEL_717e8a069a40436889d9efa521673020","IPY_MODEL_59274a4ef460413c9646a08f516ed248"],"layout":"IPY_MODEL_4d5ac279ec7945b1b5e51f6168e014a9"}},"b25b49e8b9f843cba7a18d7010ba3a50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_144ecfbf1b8f40b0acfd7f3e7713c605","placeholder":"​","style":"IPY_MODEL_575796e7c3074aa1b0acdd92f573abdb","value":"Global Freqs: 100%"}},"717e8a069a40436889d9efa521673020":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56181df4c3024264ad6f7f2e730125a3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41109f101fc8415f8a79355a379c73c4","value":1}},"59274a4ef460413c9646a08f516ed248":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_103e185e82f24b1389e68cc60c1ba65a","placeholder":"​","style":"IPY_MODEL_6e858217d6eb41e8b837118eeb60dbd9","value":" 1/1 [00:08&lt;00:00,  8.87s/it]"}},"4d5ac279ec7945b1b5e51f6168e014a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"144ecfbf1b8f40b0acfd7f3e7713c605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575796e7c3074aa1b0acdd92f573abdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56181df4c3024264ad6f7f2e730125a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41109f101fc8415f8a79355a379c73c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"103e185e82f24b1389e68cc60c1ba65a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e858217d6eb41e8b837118eeb60dbd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d440e16c1937470aa822d72cf740768b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c458bcc3367b4cd0bbd5e8b0ce41d2e2","IPY_MODEL_e800f641ccfe40948c4d37620ff760d6","IPY_MODEL_a36f8ae88034402db93ee60551445e26"],"layout":"IPY_MODEL_a29423e5c2894671a5c7dec5b2283c88"}},"c458bcc3367b4cd0bbd5e8b0ce41d2e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bfb42d31721400297bb2785cee9bf50","placeholder":"​","style":"IPY_MODEL_4684c539d072468db29b7b202fc0c4e2","value":"Train Data: 100%"}},"e800f641ccfe40948c4d37620ff760d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20db89e6752c4c7bba489630a2b40b6f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58c7f88fc4e043c6a0c402409e64ed3b","value":1}},"a36f8ae88034402db93ee60551445e26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96f3f6fb5bcd47579c6532a412e02654","placeholder":"​","style":"IPY_MODEL_77597d57dfd143f7a455f25118e332b0","value":" 1/1 [01:07&lt;00:00, 67.75s/it]"}},"a29423e5c2894671a5c7dec5b2283c88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bfb42d31721400297bb2785cee9bf50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4684c539d072468db29b7b202fc0c4e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20db89e6752c4c7bba489630a2b40b6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58c7f88fc4e043c6a0c402409e64ed3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96f3f6fb5bcd47579c6532a412e02654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77597d57dfd143f7a455f25118e332b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9722e273b543425cb6d5d1c5236155f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae1de79fa6004ce59ba122ad72bd4700","IPY_MODEL_c01ffcd7a4474f5e9b5d22cf1b58d9e3","IPY_MODEL_6b0d350221f04bdcaadd11d6ae0a6ac4"],"layout":"IPY_MODEL_09ccd61d264b4ee3b710dac9295a9da3"}},"ae1de79fa6004ce59ba122ad72bd4700":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c7a0d0d64dc41f98441bba2ff301e90","placeholder":"​","style":"IPY_MODEL_5b9bf4243c51447cb77346345b3007b6","value":"Test Data: 100%"}},"c01ffcd7a4474f5e9b5d22cf1b58d9e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9cba03f8d2e4ffda48ba87c6f499406","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1a2794ccc33401c8ec8b6858f0fb1cf","value":1}},"6b0d350221f04bdcaadd11d6ae0a6ac4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12ac19f21cfa4e93a18876299ce85866","placeholder":"​","style":"IPY_MODEL_db1e79e1ae3e4f3fb30af6c0b36edaf3","value":" 1/1 [00:31&lt;00:00, 31.04s/it]"}},"09ccd61d264b4ee3b710dac9295a9da3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c7a0d0d64dc41f98441bba2ff301e90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b9bf4243c51447cb77346345b3007b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9cba03f8d2e4ffda48ba87c6f499406":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1a2794ccc33401c8ec8b6858f0fb1cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12ac19f21cfa4e93a18876299ce85866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db1e79e1ae3e4f3fb30af6c0b36edaf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d9454fbbad243d78c87b247f153fdc2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3b67f14a0d54f6e8e7f6a10352afb96","IPY_MODEL_63a4f41b37394c1580282310b603c449","IPY_MODEL_16091d045a5b4bb38c6df808c480395c"],"layout":"IPY_MODEL_44f5725ba3c84171919a37f0c5e47373"}},"a3b67f14a0d54f6e8e7f6a10352afb96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ab40a3d3dc4af5a0ecf404f61ff645","placeholder":"​","style":"IPY_MODEL_50522786468e492a93b453da63ca3074","value":"Ep 1/20 [Train]:   0%"}},"63a4f41b37394c1580282310b603c449":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c26d2f9f09d54e9494ffaea749ce7c85","max":9213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93bc464bd78e4158b842b7d51b2b4879","value":36}},"16091d045a5b4bb38c6df808c480395c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50251a4538294248875c362f7f33e18b","placeholder":"​","style":"IPY_MODEL_ea1dd1629cfe47c5b7c6f7bad5ed3adb","value":" 36/9213 [00:30&lt;1:49:21,  1.40it/s]"}},"44f5725ba3c84171919a37f0c5e47373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02ab40a3d3dc4af5a0ecf404f61ff645":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50522786468e492a93b453da63ca3074":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c26d2f9f09d54e9494ffaea749ce7c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93bc464bd78e4158b842b7d51b2b4879":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50251a4538294248875c362f7f33e18b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea1dd1629cfe47c5b7c6f7bad5ed3adb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}